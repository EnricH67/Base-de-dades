{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EnricH67/Base-de-dades/blob/main/Copia_de_Extract_Text_from_PDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H1DgytE_Qkj"
      },
      "source": [
        "# Extracting Text from PDF Files\n",
        "\n",
        "Let's look at how to extract text from a PDF file, using the [`pdfx`](https://www.metachris.com/pdfx/) library in Python.\n",
        "\n",
        "First we need to install the library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tTJWC_Uq_Qky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba295b85-26b1-4c25-d936-700922f198ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfx\n",
            "  Downloading pdfx-1.4.1-py2.py3-none-any.whl (21 kB)\n",
            "Collecting chardet==4.0.0\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 17.2 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 30 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 40 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 61 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 71 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 81 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 92 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 102 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 112 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 122 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 133 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 143 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 153 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 163 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 174 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 178 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six==20201018\n",
            "  Downloading pdfminer.six-20201018-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 28.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20201018->pdfx) (2.4.0)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 37.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six==20201018->pdfx) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six==20201018->pdfx) (2.21)\n",
            "Installing collected packages: cryptography, chardet, pdfminer.six, pdfx\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 3.0.4\n",
            "    Uninstalling chardet-3.0.4:\n",
            "      Successfully uninstalled chardet-3.0.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires chardet<4,>=3.0.2, but you have chardet 4.0.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed chardet-4.0.0 cryptography-37.0.2 pdfminer.six-20201018 pdfx-1.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2DSjX3EBKvH",
        "outputId": "bbcf6c96-8061-4701-b9d1-58e40b498c6e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72-C2vj7_Qk0"
      },
      "source": [
        "Next, let's work with an example from the corpus in the [Rich Context leaderboard competition](https://github.com/Coleridge-Initiative/rclc/blob/master/corpus.ttl) – a machine learning competition about parsing named entities from PDFs of open access research publications.\n",
        "\n",
        "The following snippets in [TTL format](https://en.wikipedia.org/wiki/Turtle_(syntax)) show a research paper `publication-7aa3d69253e37668541c` hosted on [EuropePMC](https://europepmc.org/) that has a known link to a dataset `dataset-0a7b604ab2e52411d45a` hosted by the [Centers for Disease Control and Prevention](https://wwwn.cdc.gov/nchs/nhanes/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg2leEAv_Qk5"
      },
      "source": [
        "```\n",
        ":publication-7aa3d69253e37668541c\n",
        "  rdf:type :ResearchPublication ;\n",
        "  foaf:page \"http://europepmc.org/articles/PMC3001474\"^^xsd:anyURI ;\n",
        "  dct:publisher \"PLoS One\" ;\n",
        "  dct:title \"VKORC1 common variation and bone mineral density in the Third National Health and Nutrition Examination Survey\" ;\n",
        "  dct:identifier \"10.1371/journal.pone.0015088\" ;\n",
        "  :openAccess \"http://europepmc.org/articles/PMC3001474?pdf=render\"^^xsd:anyURI ;\n",
        "  cito:citesAsDataSource :dataset-0a7b604ab2e52411d45a ;\n",
        ".\n",
        "\n",
        ":dataset-0a7b604ab2e52411d45a\n",
        "  rdf:type :Dataset ;\n",
        "  foaf:page \"https://wwwn.cdc.gov/nchs/nhanes/\"^^xsd:anyURI ;\n",
        "  dct:publisher \"Centers for Disease Control and Prevention\" ;\n",
        "  dct:title \"National Health and Nutrition Examination Survey\" ;\n",
        "  dct:alternative \"NHANES\" ;\n",
        "  dct:alternative \"NHANES I\" ;\n",
        "  dct:alternative \"NHANES II\" ;\n",
        "  dct:alternative \"NHANES III\" ;\n",
        ".\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUmEjuic_Qk8"
      },
      "source": [
        "The paper is:\n",
        "\n",
        "  * [\"VKORC1 common variation and bone mineral density in the Third National Health and Nutrition Examination Survey\"](http://europepmc.org/articles/PMC3001474); Dana C. Crawford, Kristin Brown-Gentry, Mark J. Rieder; _PLoS One_. 2010; 5(12): e15088.\n",
        "\n",
        "We'll used `pdfx` to download the PDF file directly from the open access URL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJwtaGIE_Qk-"
      },
      "outputs": [],
      "source": [
        "import pdfx\n",
        "\n",
        "pdf = pdfx.PDFx(\"http://europepmc.org/articles/PMC3001474?pdf=render\")\n",
        "\n",
        "pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMzgGFaG_QlA"
      },
      "source": [
        "Next, use the `get_text()` function to extract the text from the `pdf` object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MchMShXp_QlC"
      },
      "outputs": [],
      "source": [
        "text = pdf.get_text()\n",
        "text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cooa0VmY_QlF"
      },
      "source": [
        "Now we can use `spaCy` to parse that text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVWD2nWi_QlG"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFSHBE0e_QlJ"
      },
      "source": [
        "Let's look at a dataframe of the parsed tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khfZtroR_QlL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "cols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")\n",
        "rows = []\n",
        "\n",
        "for t in doc:\n",
        "    row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]\n",
        "    rows.append(row)\n",
        "\n",
        "df = pd.DataFrame(rows, columns=cols)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLTyrWoJ_QlN"
      },
      "source": [
        "The parsed text shows lots of characters that could be cleaned up, but for this demo, let's run *named entity resolution* in `spaCy` to extract the entities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvv9sKCH_QlW"
      },
      "outputs": [],
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKSMdRge_Qlj"
      },
      "source": [
        "Great, that identified multiple mentions of the _NHANES_ dataset:\n",
        "\n",
        "  * `the Third National Health and Nutrition Examination Survey` _ORG_\n",
        "  * `NHANES III` _PERSON_\n",
        "  \n",
        "The default labels aren't correct, but we could [update the Named Entity Recognizer](https://spacy.io/usage/training#ner) in `spaCy` to fix that."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copia de Extract_Text_from_PDF.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}